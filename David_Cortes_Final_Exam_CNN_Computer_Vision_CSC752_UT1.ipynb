{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWpxTojKKJs7eWzIfC/u4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josedavidcortes-com/finalComputerVision/blob/main/David_Cortes_Final_Exam_CNN_Computer_Vision_CSC752_UT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsL37oNa5Krl",
        "outputId": "9cfe14b4-8c4c-4eac-9ed7-cbba6d4c3a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets get some libraries!\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "#CNN\n",
        "from keras.layers import Conv2D, MaxPool2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Dense, Flatten, Input, Lambda\n",
        "\n",
        "#tensorflow <3\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import itertools\n",
        "\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau , EarlyStopping\n",
        "\n",
        "#those curly yellow underlines are a very known error, please check the next link for more information\n",
        "#https://stackoverflow.com/questions/73358867/import-tensorflow-keras-optimizers-could-not-be-resolvedreportmissingimports\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import optimizers, losses\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#can I use my Christmas gift (NVIDIA GPU) to improve the time? Lets see!!\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as trans\n",
        "\n",
        "#do we have a GPU? this is interesting, Im going to try this on local.\n",
        "print('PyTorch Version: ', torch.__version__)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "WJzpsS7c5evC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa982e5-6573-440e-fea8-9e5a78d13172"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.13.0+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets load the dataset\n",
        "#training folder path\n",
        "train_dir = '/content/drive/MyDrive/shoe_dataset/train/'\n",
        "#testing folder path\n",
        "test_dir = '/content/drive/MyDrive/shoe_dataset/test/'"
      ],
      "metadata": {
        "id": "h4DiX7sT5kjY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate batches of tensor image data with real-time data augmentation. \n",
        "#Keras ImageDataGenerator is used for getting the input of the original data and further, it makes the transformation of \n",
        "#this data on a random basis and gives the output resultant containing only the data that is newly transformed. \n",
        "#It does not add the data. Keras image data generator class is also used to carry out data augmentation where we aim to gain \n",
        "#the overall increment in the generalization of the model. Operations such as rotations, translations, shearin, scale changes, \n",
        "# and horizontal flips are carried out randomly in data augmentation using an image data generator.\n",
        "#for more information please check the next link:\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "trn_data = ImageDataGenerator( rescale = 1./ 255 , rotation_range = 35 , width_shift_range = 0.2, height_shift_range = 0.2 , shear_range = 0.2, zoom_range = 0.2 , horizontal_flip = True, fill_mode = 'nearest')"
      ],
      "metadata": {
        "id": "lcYrG4gK9Pbh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data = ImageDataGenerator( rescale = 1./ 255 )"
      ],
      "metadata": {
        "id": "xZGmRd16AeIt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trn_dt = trn_data.flow_from_directory( directory = train_dir , batch_size = 32 , target_size = ( 240 , 240 ) , class_mode = \"categorical\" , shuffle = False)\n",
        "\n",
        "tst_dt = tst_data.flow_from_directory( directory = test_dir  , batch_size = 32 , target_size = ( 240 , 240 ) , class_mode = \"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACIQPMi8AjXI",
        "outputId": "2624e5a5-45e9-4418-bf63-dc0006e304e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 711 images belonging to 3 classes.\n",
            "Found 114 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    Conv2D( 16, (3,3) , activation = 'relu'  , input_shape = ( 240 , 240 , 3 ) ) , MaxPooling2D( 2 , 2 ),\n",
        "    Conv2D( 32, (3,3) , activation = 'relu') , MaxPooling2D(2,2) ,\n",
        "    Conv2D( 32, (3,3) , activation = 'relu') , MaxPooling2D(2,2) , Conv2D( 32, ( 3 , 3 ), activation = 'relu' ) , MaxPooling2D( 2 , 2 ) ,\n",
        "    Flatten(),\n",
        "    Dense( 512, activation = 'relu') , Dropout(0.2) , Dense( 3 , activation = 'softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "SPDi9-y0AqJ7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Adam Optimizer to set the learning rate of our final model\n",
        "opt = optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compiling and setting the parameters we want our model to use\n",
        "model.compile( loss = \"categorical_crossentropy\" , optimizer = opt , metrics = ['accuracy'] )"
      ],
      "metadata": {
        "id": "pwFvc3XNAz23"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(trn_dt, epochs = 25 , steps_per_epoch = len(trn_dt) , validation_data = tst_dt , validation_steps = int ( 0.25 * len( tst_dt ) ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkHhJEdZBV_r",
        "outputId": "d5b36923-7687-443c-e8ec-f79a9b7b7508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "23/23 [==============================] - 53s 2s/step - loss: 1.2483 - accuracy: 0.3558 - val_loss: 1.1539 - val_accuracy: 0.3438\n",
            "Epoch 2/25\n",
            "23/23 [==============================] - 48s 2s/step - loss: 1.1740 - accuracy: 0.3179 - val_loss: 1.1244 - val_accuracy: 0.2500\n",
            "Epoch 3/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.1129 - accuracy: 0.2925 - val_loss: 1.0962 - val_accuracy: 0.2812\n",
            "Epoch 4/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.1079 - accuracy: 0.3361 - val_loss: 1.0960 - val_accuracy: 0.3125\n",
            "Epoch 5/25\n",
            "23/23 [==============================] - 43s 2s/step - loss: 1.1025 - accuracy: 0.2911 - val_loss: 1.1050 - val_accuracy: 0.3438\n",
            "Epoch 6/25\n",
            "23/23 [==============================] - 44s 2s/step - loss: 1.0987 - accuracy: 0.3165 - val_loss: 1.1004 - val_accuracy: 0.3750\n",
            "Epoch 7/25\n",
            "23/23 [==============================] - 50s 2s/step - loss: 1.1049 - accuracy: 0.3263 - val_loss: 1.0925 - val_accuracy: 0.3438\n",
            "Epoch 8/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.0885 - accuracy: 0.3446 - val_loss: 1.0435 - val_accuracy: 0.4375\n",
            "Epoch 9/25\n",
            "23/23 [==============================] - 44s 2s/step - loss: 1.0921 - accuracy: 0.3361 - val_loss: 1.0933 - val_accuracy: 0.4375\n",
            "Epoch 10/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.0950 - accuracy: 0.3460 - val_loss: 1.1245 - val_accuracy: 0.3125\n",
            "Epoch 11/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.0919 - accuracy: 0.3474 - val_loss: 1.0406 - val_accuracy: 0.4375\n",
            "Epoch 12/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.0904 - accuracy: 0.3263 - val_loss: 1.0899 - val_accuracy: 0.3438\n",
            "Epoch 13/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.0866 - accuracy: 0.3685 - val_loss: 1.0416 - val_accuracy: 0.5312\n",
            "Epoch 14/25\n",
            "23/23 [==============================] - 44s 2s/step - loss: 1.0808 - accuracy: 0.4065 - val_loss: 1.1067 - val_accuracy: 0.4062\n",
            "Epoch 15/25\n",
            "23/23 [==============================] - 42s 2s/step - loss: 1.0780 - accuracy: 0.4093 - val_loss: 1.0697 - val_accuracy: 0.4062\n",
            "Epoch 16/25\n",
            "15/23 [==================>...........] - ETA: 13s - loss: 1.0947 - accuracy: 0.3890"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets see if the layers of the model are correct\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3cy8nnPOBgH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "metadata": {
        "id": "sSx0ovBSBg8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the plot\n",
        "plt.subplot( 2 , 1 , 2 )\n",
        "plt.plot( loss , label = 'Training Loss' )\n",
        "plt.plot( val_loss , label = 'Test Loss' )\n",
        "plt.legend( loc = 'upper right' )\n",
        "plt.ylabel( 'Cross Entropy' )\n",
        "plt.ylim( [ 0 , max (plt.ylim( ) ) ] )\n",
        "plt.title( 'Training and Test Loss' )\n",
        "#and ...voilÃ , c'est magnifique!\n",
        "plt.show( )"
      ],
      "metadata": {
        "id": "H2QoTlAWBmRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history [ 'accuracy' ]\n",
        "val_acc = history.history [ 'val_accuracy' ]"
      ],
      "metadata": {
        "id": "wJINdOJZBpCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 8 , 8 ) )\n",
        "plt.subplot( 2 , 1 , 1 )\n",
        "plt.plot(acc, label = 'Training Accuracy' )\n",
        "plt.plot( val_acc , label = 'Testing Accuracy' )\n",
        "plt.legend( loc = 'lower right' )\n",
        "plt.ylabel( 'Accuracy' )\n",
        "plt.ylim( [min( plt.ylim( ) ) , 1] )\n",
        "plt.title( 'Training and Test Accuracy' )"
      ],
      "metadata": {
        "id": "PmoAnyaDBud1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss and accuracy curves for training and validation \n",
        "fig , ax = plt.subplots ( 2 , 1 )\n",
        "ax[0].plot(history.history[ 'loss' ] , color='b' , label = \"Training loss\")\n",
        "ax[0].plot(history.history['val_loss'] , color='r' , label = \"validation loss\" ,axes = ax[ 0 ] )\n",
        "legend = ax[0].legend( loc = 'best' , shadow = True )\n",
        "\n",
        "ax[1].plot( history.history['accuracy'] , color = 'b' , label = \"Training accuracy\" )\n",
        "ax[1].plot( history.history['val_accuracy'] , color = 'r' , label = \"Validation accuracy\" )\n",
        "legend = ax[ 1 ].legend( loc = 'best' , shadow = True )"
      ],
      "metadata": {
        "id": "urkQ3HdhJbab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at confusion matrix \n",
        "\n",
        "def plot_confusion_matrix( cm , classes , normalize=False , title='Confusion matrix' , cmap = plt.cm.Blues ):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "#Y_pred = model.predict(X_val)\n",
        "Y_pred = model.predict(tst_dt)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax( tst_dt , axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(10))"
      ],
      "metadata": {
        "id": "Sezx88OIJMvr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}